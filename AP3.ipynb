{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AP3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 3 Mikołaj Łuczak"
      ],
      "metadata": {
        "id": "giAugGexdzE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing modules and custom DataSet"
      ],
      "metadata": {
        "id": "M6UCQEMBPcTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MSs9CrMPJZ4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "import os.path\n",
        "from os.path import isfile\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transform\n",
        "from skimage import io\n",
        "import PIL\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "import torch.nn as nn\n",
        "\n",
        "path = \"./data\"\n",
        "folder = ['0', '1', '2', '3']\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path, folders, test=True,  transform=None):\n",
        "        self.dataPath = path\n",
        "        self.folders = folders\n",
        "        self.labels = []\n",
        "        self.flist = []\n",
        "        self.transform = transform\n",
        "        for f in folders:\n",
        "            path1 = self.dataPath+\"/\"+f\n",
        "            files = [path1+\"/\"+f for f in listdir(path1) if isfile(join(path1, f))]\n",
        "            self.flist.extend(files)\n",
        "            self.labels.extend([f]*len(files))\n",
        "        self.test = test\n",
        "        x_train, x_test, y_train, y_test = tts(self.flist, self.labels, test_size=0.33)\n",
        "        if test:\n",
        "            self.flist = x_test\n",
        "            self.labels = y_test\n",
        "        else:\n",
        "            self.flist = x_train\n",
        "            self.labels = y_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = np.asarray(self.labels[index], dtype=np.compat.long)\n",
        "        filename = self.flist[index]\n",
        "        file1 = PIL.Image.open(filename)\n",
        "\n",
        "        if self.transform:\n",
        "            file = self.transform(file1)\n",
        "\n",
        "        return file, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Training Dataloader and testing Dataloader"
      ],
      "metadata": {
        "id": "Piv0BSwBPoFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CustomDS = CustomDataset(path, folder, transform=transform.Compose([transform.ToTensor()]))\n",
        "\n",
        "#train_loader = DataLoader(CustomDS, batch_size=64,shuffle=True, num_workers=0, pin_memory=False, persistent_workers=False)\n",
        "train_loader = DataLoader(CustomDS, batch_size=64, shuffle=True)\n",
        "\n",
        "CustomDS = CustomDataset(path, folder, test=True,  transform=transform.Compose([transform.ToTensor()]))\n",
        "\n",
        "test_loader = DataLoader(CustomDS, batch_size=64, shuffle=True)\n",
        "#test_loader = DataLoader(CustomDS, batch_size=32, shuffle=True, num_workers=0, pin_memory=False, persistent_workers=False)"
      ],
      "metadata": {
        "id": "9wLFLPHAPwwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Neural Network"
      ],
      "metadata": {
        "id": "w4aOcrjyQsus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, input_depth, num_classes):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(input_depth, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv3 = nn.Conv2d(20, 20, kernel_size=3)\n",
        "    self.conv4 = nn.Conv2d(20, 30, kernel_size=3)\n",
        "\n",
        "    self.fc1 = nn.Linear(30*1*1, 30)\n",
        "    self.fc2 = nn.Linear(30, num_classes)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.downsample = nn.MaxPool2d(2)\n",
        "\n",
        "  def forward(self, data):\n",
        "    output = self.downsample(self.activation(self.conv1(data)))\n",
        "    output = self.downsample(self.activation(self.conv2(output)))\n",
        "    output = self.activation(self.conv3(output))\n",
        "    output = self.activation(self.conv4(output))\n",
        "\n",
        "    output = nn.functional.adaptive_avg_pool2d(output, (1,1))\n",
        "\n",
        "    output = output.view(-1, output.shape[1]*output.shape[2]*output.shape[3])\n",
        "\n",
        "    output = self.activation(self.fc1(output))\n",
        "    output = self.fc2(output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "43Kk9QmrQ2uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting Device, Creating model optimazer and loss"
      ],
      "metadata": {
        "id": "_d85AtJmRD-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CNN(3, 4).to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "UWWerrF9RGXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Testing Functions"
      ],
      "metadata": {
        "id": "rqfJxLt6RZCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, data):\n",
        "    model.train()\n",
        "\n",
        "    for id, (images, target) in enumerate(data):\n",
        "      images, target = images.to(device), target.to(device)\n",
        "      target=target.long()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(images)\n",
        "      # print(output1.shape)\n",
        "\n",
        "      error = (loss(output, target))\n",
        "\n",
        "      error.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if id % 10 == 0:\n",
        "        print(\"Training error %.3f\"%(error.data.item()))\n",
        "\n",
        "\n",
        "def test(data):\n",
        "  model.eval()\n",
        "  correct=0\n",
        "\n",
        "  for batch_id, (images, target) in enumerate(data):\n",
        "      images, target = images.to(device), target.to(device)\n",
        "      target = target.long()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        output = model(images)\n",
        "\n",
        "        predicted = output.data.max(1)[1]\n",
        "\n",
        "        correct+=predicted.eq(target.data).cpu().sum()\n",
        "\n",
        "  print(\"Accuarcy: %3f\"%(correct/len(data.dataset)))\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "v5KdEqcXRgk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling out a train and test function"
      ],
      "metadata": {
        "id": "DfE8qMo5R4DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#epochs = 100\n",
        "epochs = 10\n",
        "for i in range(epochs):\n",
        "    train(1, train_loader)\n",
        "\n",
        "test(test_loader)"
      ],
      "metadata": {
        "id": "pBfHIwxPRlrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I had some problems with this assignment but I finally manage to do it.\n"
      ],
      "metadata": {
        "id": "oErkHNdJSLA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am very sorry for the delay"
      ],
      "metadata": {
        "id": "iWLoJvT3dALW"
      }
    }
  ]
}